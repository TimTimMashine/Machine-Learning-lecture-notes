### Идея
 Градиентный спуск Нестерова + moment

 Считаем градиент не в текущей точке, а как бы в будущей (предсказанной точке). Если впереди нам нужно будет идти в обратную сторону, то будем замедляться быстрее.

Сохранем свойство момента, но делаем замедление.

$$ 
\displaylines{

\text{grad} \, f(\vec{x}) = \left\{ \frac{\partial f}{\partial x_i} \right\}_{i=1}^n \\


\vec{x} = \{x_1, x_2, x_3, x_4, \dots\} \\

\text{Moment + Nesterov}\\

m = 0, \quad x = 0, \\
 \quad \alpha \in [0,1], \quad \alpha \rightarrow 1 \\

\text{for } i = 1 \text{ to } n:
\quad \\

 ~~~~~~~~~~~~~~~~ g = \text{grad} \, (f(x_i-\gamma\cdot m)) \\  ~~~~~~~~~~~~~~~m = \alpha \cdot m + (1 - \alpha) \cdot g \\

~~~~~x_{i}= x_{i-1} - \gamma\cdot m \\

}


$$
![[Pasted image 20250413224544.png]]



Классический алгоритм Нестерова для минимизации функции (неточно, но подходит для задачи):  
![[Pasted image 20241110212329.png]]

![[Pasted image 20241110212316.png]]

![[Pasted image 20241110212353.png]]

![[Pasted image 20241110212407.png]]

---------------------

Метод ускоренного градиента Нестерова (Nesterov Accelerated Gradient, NAG) является улучшением стандартного метода градиентного спуска, где используется информация о предыдущих шагах для более быстрого схода. Вот основные формулы и идеи, которые лежат в основе этого алгоритма.


![[Pasted image 20241110171226.png]]

![[Pasted image 20241110171317.png]]

![[Pasted image 20241110171422.png]]

![[Pasted image 20241110171444.png]]

###### Заключение

Алгоритм ускоренного градиентного спуска Нестерова использует два ключевых момента:

- **Предсказание** следующей точки с учётом информации о предыдущих шагах.
- **Моментум**, который помогает корректировать шаги, делая их более направленными и ускоряя сходимость.

С помощью этих шагов метод Нестерова значительно быстрее сходится к минимуму, чем стандартный градиентный спуск, особенно на выпуклых функциях.

----

![[Pasted image 20250204222645.png]]

полная формула из видео