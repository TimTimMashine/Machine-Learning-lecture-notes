
1. [[Функция потерь. Оптимизация.]]
2. [[Производная, частные производные, градиент. Методы оценки градиента.]]
3. [[Градиентный спуск]]
4. [[Стохастический градиентный спуск]]
5. [[Проблема локального минимума и выбора шага|Проблема локального минимума]] (и выбора learnin rate)
6. [[Метод момента]]
7. Проблема метода момента (долго останавливаемся)
8. [[Метод Нестерова]]
9. [[Проблема разных единиц измерения]]
10. [[Adagrad, нормирование градиента]]
11. Проблема Adagrad
12. [[Adadelta]]
13. [[RMSprop]] - решение проблемы Adagrad
14. [[Adam]]
15. [[Критерии остановки градиентного спуска]]

Дописать про частные производные, там нет картинок

- [+] Функция потерь. Оптимизация.
- [ ] Производная, частные производные, градиент. Методы оценки градиента.
- [ ] Градиентный спуск, проблема выбора шага.
- [ ] Стохастический градиентный спуск. 
- [ ] Использование момента. Метод Нестерова.
- [ ] Adagrad, Adadelta, RMSProp, Adam.

