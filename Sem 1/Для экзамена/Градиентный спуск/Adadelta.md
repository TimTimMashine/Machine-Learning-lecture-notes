
### Adadelta

цель: управлять шагом сходимости

Adadelta — это метод оптимизации, который автоматически адаптирует шаг обучения, используя экспоненциальные скользящие средние квадратов градиентов и обновлений параметров. Это позволяет избежать проблем с выбором фиксированного learning rate и обеспечивает более стабильную и быструю сходимость, особенно в случаях, когда градиенты сильно варьируются по масштабу.

------------

- **Адаптивное управление шагом обучения:**  
    Вместо того чтобы использовать фиксированный global learning rate, Adadelta динамически корректирует шаг обучения для каждого параметра. Это достигается за счёт учета двух вещей:
    
    - Экспоненциального скользящего среднего квадратов градиентов (подобно RMSprop).
    - Экспоненциального скользящего среднего квадратов обновлений параметров.
    
- **Отсутствие необходимости задавать глобальный learning rate**

    В Adadelta выбор параметра learning rate практически отпадает, так как алгоритм вычисляет относительный шаг обновления, который корректируется через накопленные значения градиентов и обновлений. Это особенно полезно, когда градиенты сильно варьируются по масштабу.

-----------


![[Pasted image 20250205002204.png]]

Здесь также начальные значения инициализируются нулем.

![[Pasted image 20250205002326.png]]

**Пояснение**
![[Pasted image 20250205003010.png]]

-------------------------------------------------------------------------------------------------------

![[Pasted image 20250205002419.png]]

-------

Начальные значения инициализирутся нулём.

![[Pasted image 20250205002659.png]]

1. Сначала вычисляем G как в RMSprop от квадр. градиентов
2. Затем дельта - это эксп. скользящее средняя нормированных кврд. градиентов
3. Вычисляем 2 строчку


![[Pasted image 20250205002836.png]]

и обновляем веса




