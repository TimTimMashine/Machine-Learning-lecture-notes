
# Функция потерь

**Функция потерь** (или **функция ошибок**) — это математическое выражение, которое используется в статистике и машинном обучении для оценки качества модели. Она измеряет, насколько предсказанные значения (выходы модели) отличаются от фактических (истинных) значений.

Зачем нужна функция потерь?

Функция потерь позволяет:

1. Оценить качество модели: Чем меньше значение функции потерь, тем лучше модель справляется с задачей.

2. Оптимизировать модель: В процессе обучения модели используется функция потерь для настройки параметров (например, весов нейронной сети) с помощью алгоритмов оптимизации (например, градиентного спуска).
3. 

![[Pasted image 20241031011909.png]]
## MAE
**MAE** - (Mean Absolute Error, средняя абсолютная ошибка) — это одна из функций потерь, используемая для оценки качества моделей регрессии. Она измеряет, насколько предсказанные значения отклоняются от истинных значений, вычисляя среднее абсолютное значение этих отклонений.


![[Pasted image 20241031012112.png]]

### Преимущества MAE

1. **Простота интерпретации**: MAE легко понять, так как она измеряет среднюю величину ошибок в тех же единицах, что и предсказания.
2. **Устойчивость к выбросам**: В отличие от MSE, MAE менее чувствителен к выбросам, так как использует абсолютные значения, а не квадрат ошибок.

Недостатки

1. **Не дифференцируема в точках**: MAE не имеет производной в точках, где ошибки равны нулю, что может усложнить оптимизацию.
2. **Не штрафует большие ошибки**: MAE одинаково рассматривает все ошибки, независимо от их величины, что может быть недостатком в некоторых задачах.
## Функция потерь дополнение

Функция потерь и оптимизация – это ключевые концепции в машинном обучении, особенно в задачах, связанных с обучением моделей.

Функция потерь измеряет, насколько хорошо модель предсказывает выходные данные. Чем меньше значение функции потерь, тем лучше модель справляется с задачей.

- **Для регрессии**
    
    - MSE (Mean Squared Error) – среднеквадратичная ошибка
    - MAE (Mean Absolute Error) – средняя абсолютная ошибка
    - Huber Loss – гибрид MSE и MAE, устойчивый к выбросам
- **Для классификации**
    
    - Логистическая функция потерь (Log Loss) – используется в логистической регрессии
    - Кросс-энтропия – популярна в многоклассовых задачах
    - Hinge Loss – применяется в SVM


# Оптимизация
## Основное про оптимизацию 

**Роль оптимизации** в машинном обучении заключается в том, чтобы настроить параметры модели так, чтобы она максимально точно решала поставленную задачу (например, предсказывала данные, классифицировала объекты и т.д.). 

**Задача оптимизации** — это систематический поиск минимума (или максимума) выбранной функции потерь, что позволяет модели обучаться и улучшаться с каждой итерацией.


**Оптимизация** — это поиск такого набора переменных (например, параметров модели), при котором некоторая функция достигает своего наилучшего значения. Обычно это означает:

- **Минимизация функции:** поиск минимального значения (например, минимизация ошибки модели).
- **Максимизация функции:** поиск максимального значения (например, максимизация прибыли).

#### Роль оптимизации в машинном обучении

В машинном обучении основная цель — создать модель, которая хорошо предсказывает или классифицирует данные. Для этого необходимо настроить параметры модели (например, веса нейронной сети). Роль оптимизации здесь состоит в следующем:

1. **Определение функции потерь:**  
    Функция потерь (loss function) измеряет, насколько точны предсказания модели. Чем меньше значение функции потерь, тем лучше модель.
    
2. **Настройка параметров модели:**  
    Оптимизационный алгоритм корректирует параметры модели так, чтобы значение функции потерь стало как можно меньше. Это позволяет модели лучше соответствовать обучающим данным и, надеясь, обобщать на новые.
    
3. **Обеспечение эффективного обучения:**  
    Правильно выбранный метод оптимизации помогает быстрее и стабильнее сходиться к наилучшему решению, а также предотвращает застревание в неудачных точках (локальных минимумах).

![[Pasted image 20250204160906.png]]

![[Pasted image 20250204160957.png]]

#### Как осуществляется оптимизация?

На практике используются различные алгоритмы оптимизации:

- **Градиентный спуск:**  
    На каждом шаге алгоритм вычисляет градиент (направление наибыстрейшего возрастания ошибки) и корректирует параметры в противоположном направлении, чтобы уменьшить ошибку.
    
- **Адаптивные методы (Adam, RMSprop, AdaGrad):**  
    Эти алгоритмы автоматически регулируют скорость обучения для каждого параметра, что помогает быстрее и стабильнее сходиться к оптимальному решению.
    
- **Другие подходы:**  
    Могут использоваться эвристические или метаэвристические методы, особенно для сложных или нестандартных задач.


------------

## Средний эмпирический риск

Задачу машинного обучения можно рассматривать как задачу оптимизации, а именно, задачу **минимизации среднего эмпирического риска**. Давай разберем, как минимизировать этот риск и как в этом помогают производные, в частности частные.

Средний эмпирический риск — это функция, которая показывает, насколько хорошо модель предсказывает значения на обучающих данных. Его минимизация помогает нам найти такие параметры модели, при которых ошибка предсказания будет наименьшей.

**Средний эмпирический риск** — это мера, используемая в статистическом обучении для оценки качества модели на обучающих данных. Он представляет собой среднее значение функции потерь по всем наблюдениям в обучающем наборе. Другими словами, это усреднённая ошибка модели, вычисленная на реальных данных, которую мы пытаемся минимизировать в процессе обучения.


![[Pasted image 20241109001501.png]]

##### 2. Способы минимизации среднего эмпирического риска

###### Градиентный спуск

Наиболее распространенный метод минимизации среднего эмпирического риска — **градиентный спуск**. Этот метод итеративно обновляет параметры модели, чтобы постепенно приближаться к минимуму функции потерь.

![[Pasted image 20241109001603.png]]

###### Роль производных

Производные играют ключевую роль в градиентном спуске, поскольку они показывают, как сильно изменяется функция при небольшом изменении каждого параметра. Если значение производной положительное, это значит, что функция растет при увеличении параметра, а если отрицательное — функция убывает. Градиентный спуск использует производные, чтобы двигаться к точке, где функция потерь минимальна.

![[Pasted image 20241109001807.png]]

###### 4. Другие методы минимизации

Помимо градиентного спуска, существуют и другие методы минимизации:

- **Стохастический градиентный спуск (SGD)**: Вместо вычисления градиента по всей выборке каждый раз берется только один случайный пример для обновления параметров, что ускоряет процесс.
- **Мини-батч градиентный спуск**: Градиент рассчитывается на небольшой группе данных (батче), что делает метод стабильнее по сравнению с SGD.
- **Методы второго порядка**: Используют не только градиенты, но и **вторые производные** (например, метод Ньютона), чтобы точнее оценивать направление и шаг. Однако они сложнее и затратнее в вычислительном плане.

###### Вкратце:

4. Минимизация среднего эмпирического риска — это задача оптимизации, и производные используются для вычисления направления, в котором функция потерь будет убывать.
5. Частные производные позволяют рассчитать изменение функции по каждому параметру отдельно, что помогает найти наилучшие параметры для минимизации ошибки.

![[Pasted image 20250204161335.png]]

...
