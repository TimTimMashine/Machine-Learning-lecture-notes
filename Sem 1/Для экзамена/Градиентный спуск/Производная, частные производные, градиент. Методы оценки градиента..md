### Общая инфа про производные
$$\displaystyle \begin{cases}
f(x)_{x}^{'}=\dfrac{f(x+\triangle) - f(x)}{\triangle} \\\\ \triangle \rightarrow 0 \end{cases} $$
$$\displaystyle \begin{cases}
f(x_{min}) = min \\\\ f^{'}(x_{min})= 0 \end{cases} $$
$\displaystyle f'(x_{max}) = 0$
$\displaystyle f'(x_{soiddle}) = 0$ - седловая точка/точка перегиба
![[Pasted image 20241031224546.png]]

![[Pasted image 20241031224636.png]]

![[Pasted image 20241031224652.png]]

...

--------------------------------------------------------------------------------------------------------------------
### Частная производная

#Частные_производные

Частные производные — это основное понятие в многомерном анализе, которое позволяет исследовать, как функция изменяется относительно одной переменной при фиксированных значениях остальных переменных. Давай подробно рассмотрим, что такое частные производные, как их вычислять, и их применение.

Делаем вид, что все остальные переменные, кроме рассматриваемой константы.

![[Pasted image 20241101004956.png]]

![[Pasted image 20241101004853.png]]

![[Pasted image 20241101004917.png]]

![[Pasted image 20241101004942.png]]

-------------------------
Смысл производной в том, что она показывает как изменяется функции, то есть это скорость изменения функции.
$$\displaystyle Если ~ \dfrac{\delta f}{\delta x_1} = a $$
$$\displaystyle то ~ писать ~ {\delta f} = a \cdot {\delta x_1} ~ - ~ нельзя$$
### Градиент

![[Pasted image 20250204153050.png]]

![[Pasted image 20250204153554.png]]

![[Pasted image 20250204153656.png]]

### Методы оценки градиента

![[Pasted image 20250204154155.png]]

![[Pasted image 20250204154211.png]]

![[Pasted image 20250204154414.png]]


![[Pasted image 20250204154140.png]]

![[Pasted image 20250204154225.png]]
![[Pasted image 20250204154355.png]]


![[Pasted image 20250204154320.png]]

![[Pasted image 20250204154300.png]]

#### Дополнительная пара методов оптмизации

- **Метод наискорейшего спуска (градиентный спуск):**  
    На каждом шаге выбирается направление, противоположное градиенту функции, то есть направление наибыстрейшего уменьшения. Цель – итеративно уменьшать значение функции, приближаясь к минимуму.
    
- **Метод покоординатного спуска:**  
    Вместо одновременной оптимизации по всем переменным метод оптимизирует функцию по одной координате (или группе координат) за раз, фиксируя остальные. Цель – упростить задачу оптимизации, особенно когда по отдельным переменным задачу можно решить аналитически или быстрее.
    
- **Метод сопряжённых градиентов:**  
    Использует специальные «сопряжённые» направления, которые улучшают эффективность поиска минимума для квадратичных функций (или их приближений). Цель – добиться более быстрого сходимости, особенно для больших разреженных систем, чем при использовании простого градиентного спуска.


все перечисленные методы относятся к классу методов оптимизации. Их основная цель — найти экстремум (минимум или максимум) целевой функции, обычно через итеративное приближение с использованием информации о градиенте или отдельных координатах.