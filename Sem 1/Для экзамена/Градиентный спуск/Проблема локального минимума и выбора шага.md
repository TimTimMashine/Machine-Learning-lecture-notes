
### Проблема локального минимума

Можем скатиться в локальный минимум вместо глобального

### Проблема выбора шага

Проблема выбора шага (learning rate) — это один из ключевых аспектов алгоритмов градиентного спуска. Правильное определение размера шага критически важно, потому что оно напрямую влияет на сходимость и стабильность процесса оптимизации. Вот несколько аспектов, почему выбор шага столь важен:

Дивергенция - это когда алгоритм не сходиться к решению.

1. **Слишком большой шаг:**
    
    - **Перескок через минимум:** Если шаг выбран слишком большим, обновления параметров будут слишком резкими. Это может привести к тому, что алгоритм «перескочит» через точку минимума и никогда не сможет стабилизироваться около оптимального значения.
    - **Колебания и дивергенция:** При больших шагах значения функции могут колебаться или даже увеличиваться, что приводит к нестабильной работе алгоритма и возможной дивергенции (то есть алгоритм вовсе не сходится к решению).
    
2. **Слишком маленький шаг:**
    
    - **Медленная сходимость:** Если шаг слишком мал, каждое обновление будет вносить лишь незначительные изменения в параметры. В результате, алгоритм будет сходиться очень медленно, требуя большого числа итераций для достижения приемлемого результата.
    - **Застревание в локальных минимумах:** При маленьких шагах алгоритм может оказаться менее «динамичным», что увеличивает риск застревания в локальных минимумах, особенно если функция потерь имеет множество плоских или почти плоских участков.

---------

*1. Есть стандартный подход, где начинают с 0.1, 0.01, 0.001 и каждый раз переходят на порядок меньше.

*2. Learning rate schedules (расписание изменения шага, то есть тоже динамический шаг):

- Часто применяют стратегии, при которых размер шага уменьшается по мере приближения к минимуму. Это позволяет делать крупные шаги в начале (быстро приблизиться к оптимальному значению), а затем переходить к мелким корректировкам для более точной настройки параметров.

Расчет лямбды
Лямбда здесь - это шаг сходимости алгоритма
![[Pasted image 20241031231637.png]]

![[Pasted image 20241101010639.png]] 
Так в python выглядит подсчет шага (mn заранее определенна на 100)

*3. Вариант с нормализацией градиента, величина градиента не будет влиять, а только его знак, но проблему с шагом это полность не решает.

Нормализация градиента 3 вариант подсчета

![[Pasted image 20241101010247.png]]

Примерно так это выглядело бы в python
![[Pasted image 20241101010431.png]]]]

lmd - шаг сходимости
np.sign - это нормализация градиента
df(xx) - это заранее опредленная функция из python где происходит подсчет производной нашей функции из математики
xx - это текущая точка (вроде бы)


*4. Адаптивные методы и методы с импульсом.

- **Адаптивные методы:** Чтобы решить проблему выбора фиксированного шага, используются адаптивные методы оптимизации (например, Adam, RMSprop, AdaGrad). Они динамически подбирают размер шага для каждого параметра, учитывая историю градиентов. Это помогает быстрее сходиться, особенно на начальных этапах, и при этом сохранять стабильность при приближении к минимуму.

- **Методы с импульсом (Momentum):** Введение импульса позволяет накапливать предыдущие обновления и помогает алгоритму преодолевать небольшие локальные минимумы, способствуя более гладкому и эффективному спуску.
---

*5. Правило выбора шага сходимости из видео про SGD 

![[Pasted image 20250204204122.png]]

есть ещё такое правило выбора лямбды, N - означет сколько мы примерно будем учитывать предыдущих значений