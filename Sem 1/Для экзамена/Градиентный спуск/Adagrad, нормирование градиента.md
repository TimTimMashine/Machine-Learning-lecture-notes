### Идея
 
Т.к. есть [[Проблема разных единиц измерения]], то будем нормировать градиент

g^2 и  g / v- это поэлементно

$$ 
\displaylines{

\text{grad} \, f(\vec{x}) = \left\{ \frac{\partial f}{\partial x_i} \right\}_{i=1}^n \\


\vec{x} = \{x_1, x_2, x_3, x_4, \dots\} \\

\text{Adagrad}\\

\vec{v} = 0, \quad \vec{x_0} = 0, \\
 \quad \alpha \in [0,1], \quad \alpha \rightarrow 1 \\

\text{for } i = 1 \text{ to } n:
\quad \\

 ~~~~~~~~~~~~~~~~ g = \text{grad} \, (f(x_{i-1})) \\  ~~~~~~~~ v = v +  \alpha \cdot g ^ 2 \\

~~~~~~~~~~~~~~x_{i}= x_{i-1} - \gamma\cdot \dfrac{g}{v} \\

}


$$
![[Возможно Adagrad.png]]
### Проблема
v  всегда только увеличивается -> стремится к бесконечности и мы будем отнимать от икса ноль.


Adagrad (Adaptive Gradient) — это адаптивный метод оптимизации, который автоматически настраивает шаг обучения (learning rate) для каждого параметра индивидуально на основе накопленной информации о прошлых градиентах. Его основная идея заключается в том, чтобы делать более крупные обновления для редко встречающихся (и, возможно, менее "важных") признаков и меньшие обновления для часто встречающихся признаков, что помогает справляться с разреженными данными и различными масштабами параметров.

η — фиксированный шаг обучения для всех параметров.

Проблемы, которые могут возникнуть:

- **Разреженные данные.** Некоторые параметры (например, веса, соответствующие редко встречающимся признакам) обновляются слишком медленно.
- 
- **Различные масштабы.** Если градиенты для разных параметров имеют разные масштабы, один и тот же η может быть слишком большим для одних параметров и слишком маленьким для других.

Adagrad предлагает изменять шаг обучения для каждого параметра в зависимости от его "истории" — суммарной информации о квадрате градиентов, накопленной за все предыдущие итерации. Таким образом, если для какого-то параметра градиенты постоянно большие, его effective learning rate будет уменьшаться, а если градиенты маленькие, effective learning rate останется относительно высоким.


-------
![[Pasted image 20250204234647.png]]
Получается что для каждого параметра мы накапливаем его градиенты, от начала обучения до интерации t

А далее нормируем шаг, на корень из накопленных градиентов

![[Pasted image 20250204234719.png]]

![[Pasted image 20250204235023.png]]

Adagrad — это метод, который:

- Накапливает информацию о квадрате градиентов для каждого параметра.
- Использует эту информацию для адаптивного масштабирования шага обучения.
- Позволяет параметрам с редкими обновлениями получать больший шаг, а параметрам с частыми и большими обновлениями — меньший шаг.

Таким образом, Adagrad помогает автоматически настраивать learning rate для каждого параметра, что улучшает сходимость на разреженных данных, хотя агрессивное уменьшение effective learning rate со временем может стать его слабой стороной. Для решения этой проблемы были разработаны более современные методы (например, RMSprop, Adadelta и Adam), которые пытаются сохранить адаптивность, но избежать чрезмерного уменьшения шага обучения.

Отличительный момент
***Накопление квадратов градиентов ведётся как суммарное накопление за все итерации***

Проблема такого подхода в том, что сумма Gj​ постоянно растёт, что приводит к монотонному уменьшению effective learning rate. Это может привести к слишком медленной сходимости на поздних этапах обучения.