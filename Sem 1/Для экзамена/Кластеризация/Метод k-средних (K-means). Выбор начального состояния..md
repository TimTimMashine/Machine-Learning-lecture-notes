# Метод k-средних (K-means)

Алгоритм K-means можно описать следующими шагами:

1. **Выбор числа кластеров k. Это важный параметр, который необходимо задать заранее.
    
2. **Инициализация центров кластеров**. Выбираем k начальных центров. Это можно сделать случайным образом, но часто используется более продвинутый метод k-means++, который улучшает результаты.
    
3. **Повторяющиеся шаги (итерации)**:
    
    a. **Назначение точек к кластерам**: для каждой точки данных вычисляем расстояние до каждого из центров кластеров и присваиваем точку кластеру, в который расстояние минимально.
    
    b. **Обновление центров кластеров**: для каждого кластера пересчитываем его центр как среднее значение всех точек, принадлежащих этому кластеру.
    
4. **Проверка сходимости**: если центры кластеров не изменились (или изменения стали незначительными), алгоритм завершает выполнение

---------------------
Подробное описание

## Инициализации
### Cлучайные точки назначаются центрами кластеров
### Всем точкам назначается случайный цвет
Всем точкам назначается случайный цвет, а после в зависимости от 


### k-means++
Случайным образом раскрасим точки. 


**K-means** выполняет кластеризацию путём назначения каждой точки данных в тот кластер, центроид которого наименее удалён от этой точки. Для этого используется мера расстояния, которая в большинстве случаев является **евклидовым расстоянием**.

![[Pasted image 20250108005334.png]]

![[Pasted image 20250108005447.png]]

![[Pasted image 20250108005512.png]]

![[Pasted image 20250108005728.png]]

![[Pasted image 20250108005708.png]]

1. **Проверка сходимости**:

Процесс продолжается до тех пор, пока центры кластеров не изменятся значительно (то есть, пока разница между старыми и новыми центрами не станет меньше заданного порога), либо пока не будет достигнут максимальный номер итерации.

-----------------------


# Выбор начального состояния. K-means ++

Одной из проблем метода K-means является чувствительность к выбору начальных центров. Неправильный выбор может привести к локальным минимумам (неоптимальным решениям). Чтобы избежать этой проблемы, можно использовать метод **k-means++** для более разумного выбора начальных центров.

Основная цель метода **K-means++** — выбрать начальные центры кластеров таким образом, чтобы они были **расстояние друг от друга как можно дальше**. Это улучшает сходимость алгоритма K-means и уменьшает вероятность того, что он застрянет в локальном минимуме. В K-means++ центры выбираются с учетом того, как удалены точки от уже выбранных центров, что приводит к более разнообразным центрам и, следовательно, более качественной кластеризации.

![[Pasted image 20250108010842.png]]
#### Почему квадрат?
- Если бы вероятность была пропорциональна просто расстоянию (а не его квадрату), то новые центроиды могли бы оказаться слишком близко к уже выбранным.
    
- Квадрат расстояния **усиливает вес далёких точек**, что помогает выбирать центроиды, более равномерно покрывающие всё пространство данных.
#### Почему тогда сразу не берём самую дальнюю точку?

#####  1. Устойчивость к выбросам (задать вопрос)

Если всегда выбирать **строго самую удалённую точку**, то алгоритм станет очень чувствительным к **шумовым точкам и выбросам**. Например:

- В данных есть несколько точек, которые сильно удалены от основного кластера (аномалии).
    
- Если на каком-то шаге такая точка станет центроидом, это может испортить всю кластеризацию, так как вокруг неё почти не будет других точек.
    

**K-means++ избегает этого** благодаря вероятностному выбору:

- Да, у далёких точек больше шансов, но они не гарантированы.
##### Критика обоснования
Кажется, что всё равно эта отдельная точка либо должна стать отдельным кластером (ну и ладно), либо  к ней должны присоединиться другие и тогда центроид сместится

##### 2. Разнообразие начальных центров

Если всегда брать **строго максимальное расстояние**, то центроиды могут оказаться **слишком "крайними"**, что не всегда оптимально.

- Например, в данных с неравномерной плотностью (например, два кластера разного размера) выбор самой удалённой точки может привести к тому, что один кластер окажется разбит на части.
    
- Вероятностный выбор позволяет иногда брать точки, которые не самые далёкие, но всё же хорошо покрывают пространство.

##### 3. Теоретическая гарантия

K-means++ имеет **математически доказанное качество кластеризации** (аппроксимация O(log⁡k)O(logk) от оптимального решения).

- Если же всегда брать самую удалённую точку, такая гарантия **теряется**, и можно получить худший результат.

![[Pasted image 20250108011023.png]]

![[Pasted image 20250108011039.png]]

# Алгоритмическая сложность
$$\displaylines{
O(n\cdot k \cdot i \cdot d) \\\\

n - \text{количество точек}\\
k - \text{количество кластеров}\\
i - \text{количество операций (какиих?)}\\
d - \text{размерность (сложность вычисления функции расстояния)}\\
}$$


Можно случайно назначать кластеры точкам, есть проблема: центроиды сойдутся куда-то в центр плоскости (и алгоритм может сломаться)

# Когда k-means работает плохо? и проблемы
![[Pasted image 20250521222636.png]]
Кажется, что на ленточных кластерах сработает плохо


![[Pasted image 20250521224007.png]]

Во втором случае два кластера: высокие мальчики и все остальные

k-means, скорее всего не найдёт
![[Pasted image 20250522153108.png]]

![[Pasted image 20250522160134.png]]

Причины неудачной кластеризации:
- k-means покрывает выборку шариками, если покрытие выборки шариками не эффективно, то и k-means не эффективен

## Проблемы

Одна и таже точка может мотыляться между двумя или несколькими кластерами

При разных запусках алгоритм может сходиться к самым разным решениям

## Как понять, что решение хорошее

1. Запускаем k-means несколько раз
2. Если решение появляется часто, то, наверное, это хорошее решение
3. Как понять, что это решения похожи или совпадают: у них центры кластеров лежат на расстоянии не больше чем эпсилон

Т.е. 

(1, 2), (5, 7), (9, 10) и (1.01, 1.99), (4.98, 7.03), (8.97, 10.2) похожи

Можно заменить на расстояние без проклятия размерностей
# Как оценивается качество K-means
По сумме квадратов внутрекластерных расстояний

![[Pasted image 20250522154802.png]]

# K-means при наличии размеченных объектов (для частичного обучения)

![[Pasted image 20250522155417.png]]

Ищем кластеры не для всех объектов, а только для неразмеченных

# Другое
![[Pasted image 20250522155625.png]]
Разница: 
- выкидываем мягкую кластеризацию, заменяем на жёсткую
- выкидываем вычисление дисперсии
- не вычисляем веса кластеров

![[Pasted image 20250522160007.png]]




# Комментарии к коду из ноутбука 06.12.2025

## ячейка initialize_kmeans_pp
```
```