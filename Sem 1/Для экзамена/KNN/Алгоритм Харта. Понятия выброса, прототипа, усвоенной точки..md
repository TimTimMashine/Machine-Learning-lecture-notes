### 1. **Выброс (Outlier)**

- **Выброс** — это точка данных, которая сильно отличается от других наблюдений в наборе данных. Выбросы могут быть результатом ошибок в измерениях или же необычных, но реальных наблюдений.

- В задачах машинного обучения выбросы часто могут искажать модели, если они не обрабатываются должным образом.

- **Обработка выбросов** может включать их удаление, преобразование или использование более устойчивых алгоритмов, которые не чувствительны к выбросам.

![[Pasted image 20250201223410.png]]
### В рамках KNN

Метод k-NN для обнаружения выбросов (k-NN outlier) основывается на идее, что аномальные (выбросные) объекты обычно располагаются в разреженных областях пространства признаков. Основная концепция метода следующая:

1. **Расстояние до k-го ближайшего соседа:**  

    Для каждой точки данных вычисляется расстояние до её k-го ближайшего соседа. Это расстояние можно рассматривать как оценку локальной плотности:
    
    - Если расстояние маленькое, значит, в окрестности точки много соседей, и локальная плотность высока.
    - Если расстояние большое, то точка находится в разреженной области, что может свидетельствовать о её аномальности.
    
2. **Оценка аномальности:**  

    Чем больше расстояние до k-го ближайшего соседа, тем ниже локальная плотность вокруг точки, и, соответственно, тем выше вероятность, что данная точка является выбросом. Таким образом, расстояние до k-го соседа часто используют как outlier score.
    
3. **Порог или ранжирование:**  

    В практическом применении можно либо задать пороговое значение для расстояния (если точка имеет расстояние больше порога, то она считается аномальной), либо ранжировать объекты по величине расстояния до k-го соседа, выделяя те, у которых значение максимально.
    
4. **Сравнение с другими методами:**  

    Хотя метод достаточно прост и основан на прямых вычислениях расстояний, он часто показывает хорошую эффективность и сопоставим с другими методами обнаружения выбросов, например, с local outlier factor (LOF). LOF также оценивает локальную плотность, но сравнивает её с плотностью соседей, тогда как метод k-NN outlier напрямую использует расстояние до k-го соседа.
    

**Пример:**  
Предположим, у нас есть набор точек, и для каждой точки мы вычисляем расстояние до её 5-го ближайшего соседа. Если у одной точки это расстояние составляет 10 (например, в единицах измерения признаков), а у большинства других точек расстояния находятся в диапазоне 1–3, то эта точка, вероятно, будет считаться выбросом, так как она находится в области с низкой плотностью данных.

Таким образом, метод k-NN outlier использует расстояние до k-го ближайшего соседа как простую, но информативную меру локальной плотности, позволяющую обнаруживать аномальные объекты в данных.

### 2. **Прототип**

- **Прототип** — это представление класса или кластера данных, которое можно рассматривать как среднее или "типичное" значение для группы объектов

- В задачах классификации или кластеризации прототипы могут служить представлением для каждого класса или кластера. Например, в методах ближайших соседей или в алгоритмах кластеризации типа K-средних прототипы часто являются центроидами кластеров.

![[Pasted image 20250201223614.png]]

В контексте **алгоритма ближайших соседей**: Прототипами могут быть точки, которые наиболее хорошо представляют отдельные классы. Обычно они выбираются как те точки, которые минимизируют расстояние до других точек внутри класса.

### 3. **Усвоенная точка**

Усвоенная точка — это точка, которая была классифицирована в определенный класс или кластер на основе выбранных прототипов.

![[Pasted image 20250201223727.png]]

### 4. **Алгоритм Харта**

Алгоритм Харта (или алгоритм ближайшего прототипа) используется для классификации с использованием прототипов, где для каждого класса выбираются прототипы, и новые точки классифицируются в класс, ближайший к своему прототипу. Это метод из области обучения с учителем.

![[Pasted image 20250201223833.png]]
![[Pasted image 20250201223851.png]]


Ещё 1 вариант.


![[Pasted image 20250201225950.png]]

![[Pasted image 20250201230004.png]]

![[Pasted image 20250201230019.png]]

![[Pasted image 20250201230058.png]]

### Контекст KNN


![[Pasted image 20250201225113.png]]

![[Pasted image 20250201225137.png]]
![[Pasted image 20250201225154.png]]

### Пример с конкретными данными:

![[Pasted image 20250201224842.png]]

![[Pasted image 20250201224855.png]]

Таким образом, **алгоритм Харт** помогает улучшить классификацию в контексте **K ближайших соседей** за счет оптимизации значения K и корректировки веса для соседей, что улучшает точность классификации.


Контекст KNN


Алгоритм Харта используется для классификации и кластеризации данных. В нем данные представляются как набор точек, и задача заключается в том, чтобы определить, какие из них наиболее типичны для каждого класса. Формулы могут включать вычисление расстояний и определение "выбросов" — точек, сильно отличающихся от остальных, что может нарушать общие закономерности. Усвоенная точка в контексте алгоритма — это центральная точка или прототип, представляющий группу похожих данных. Прототип — это обобщенная точка, которая минимизирует ошибку при классификации или прогнозировании.


На странице Википедии о k-NN упоминается алгоритм "Condensed Nearest Neighbor" (CNN, алгоритм Харта) для сокращения данных. Алгоритм выбирает прототипы из обучающего набора, чтобы уменьшить объем данных, сохраняя при этом точность классификации. Он ищет элементы, чьи ближайшие прототипы имеют другой класс, и добавляет их в множество прототипов. Процесс включает вычисление "пограничных коэффициентов", которые помогают выбрать важные точки для прототипов.