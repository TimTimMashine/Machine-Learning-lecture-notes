


#### Справка по векторам
https://www.youtube.com/watch?v=mFuAkEfDuMI

Скалярное произведение это посути проекция одного вектора на другой, то есть как бы их наложение .. сравнение. (сумма произведений -> число)

![[Pasted image 20250206192243.png]]


#### SVM Вводная инфа

![[Pasted image 20250522210735.png]]


Метод опорных векторов (SVM) для **линейно разделимой** выборки строит **оптимальную разделяющую гиперплоскость**, которая максимизирует ширина полосы (L) между классами.

И наша гиперплоскость должна проходить по центру этой полосы. Чем шире полоса тем надежнее классификатор будет отделять образы одного класса от другого.

Мы исходим из того, что существует гиперплоскость которая линейно разделят образы одного класса от другого.


#### Пояснение модели

Сначала есть выражение $w^T$$x$ - $b$ 

Пояснение к параметрам

![[Pasted image 20250206195436.png]]

Вектор $w$ определяет **наклон и направление** гиперплоскости.

- Его направление — **перпендикулярно** разделяющей границе.
- Его длина $∥w∥$ влияет на ширину маржина (чем больше $∥w∥$, тем уже ширина маржин (что такое маржин?)).
- Вектор весов $w$ — это "стрелка", которая показывает направление наибольшего роста функции.
- Он всегда перпендикулярен разделяющей поверхности
- Чем больше $w_i$​, тем сильнее влияет соответствующий признак $x_i$

![[Pasted image 20250206200936.png]]

Скаляр $b$ определяет **смещение гиперплоскости** относительно начала координат.

- Если $b = 0$, гиперплоскость **проходит через начало координат**.
- Если $b > b$, гиперплоскость **смещена в сторону отрицательного класса**.
- Если $b < b$, гиперплоскость **смещена в сторону положительного класса**.

**Знак выражения**  $w^T$$x$ - $b$  указывает, по какую сторону от гиперплоскости находится точка.

Он определяется знаковой функцией $sign$

Вот так выглядит модель, где на выходе обычно метки -1; 1


![[Pasted image 20250202224806.png]]

Ширина полосы

*Ширина полосы в SVM** — это расстояние между двумя гиперплоскостями, которые проходят через поддерживающие вектора. Эти гиперплоскости параллельны и разделяют положительный и отрицательный классы.

Формула для $$L = \dfrac{2}{||w||}$$  Наша задача максимизировать маржин (L)

Здесь представленна визуализация. 

![[Pasted image 20250202224922.png]]
![[Pasted image 20250522214502.png]]
Можно посмотреть на эту задачу как на систему неравенств. 
- Если выборка линейно разделима, то система неравенств будет иметь решение.
- Если есть ошибки(выборка не разделима линейно), то система неравенств будет несовместна
- Один из видов задач (Решение) для несовместной системы неравенств - **поиск максимальной совместной подсистемы** - NP -трудная задача  
________________
- Если левую часть неравенств домножить на какую-то константу от этого ничего не изменится 
- Тогда сделаем так, чтобы максимальный отступ был равен единице

Ширина полосы, по сути определяется граничными образами первого и второго класса

Чему равна ширина полосы ![[Pasted image 20250522215708.png]]
Берём левую граничную точку и правую граничную точку, увидим, что ширина полосы равна 2 / ||w||, именно поэтому хотим минимизировать норму w
$$\displaylines{\begin{cases} \dfrac{1}{2}||w||^{2}\rightarrow min \\
 y_{i}( w\cdot x_{i} -b) \geq 1\\
 \end{cases} }\\
$$ Задача линейного программирования
![[Pasted image 20250522220716.png]]

-----------------------
![[Pasted image 20250522214230.png]]
$L_2$-регуляризация

**Разность $x_+$ - $x_-$ — это вектор, который направлен от отрицательного примера к положительному. Этот вектор можно воспринимать как вектор, который указывает на различие между двумя типами данных. Он направлен туда же куда и направлен вектор омега $w$


**Умножение $w^T$ * ($x_+$ - $x_-$)  это скалярное произведение весового вектора $w$ и разницы между положительным и отрицательным примерами. Это значение измеряет, насколько велика разница между классами относительно гиперплоскости.

Далее на рисунке мы фактически делаем проекцию вектора  $x_+$ - $x_-$ на вектор $w$

![[Pasted image 20250202225004.png]]


Далее L - ширина полосы 

Мы можем записать как скалярное произведение вектора омега ($w$) на вектор ($x_+$ - $x_-$) и деленный на норму вектор омега ($w$)

![[Pasted image 20250202225054.png]]

Делим на вектор омега для того чтобы отвязаться от вектора омега, чтобы ширина полосы не зависила от омега $w$. Если этого не делать то ширина полосы будет зависить от вектора омега.

Упрощаем уравнение. Используем не единичную норму, а квадратичную, чтобы не вычислять квадратные корни.

![[Pasted image 20250202225217.png]]

----------

#### Понятие маржин (отступ) и вывод

- Маржин - определяет насколько далеко находиться объект от разделяющей гиперплоскости. 
- отступ (margin ) объекта $x_i$ - расстояние от объекта $x_i$ до разделяющей гиперплоскости

Математчески он определяется как метка класса умноженная. на ту метку класса, которую выдает классификатор. Если метки совпадают, то классификация верная, а если не совпадают, то классификация не верная. 

![[Pasted image 20250202225317.png]]

Здесь мы пробуем нормировать величину этого отступа.

$a$ > 0 .. мы домнажем ур. на этот коээф

![[Pasted image 20250202225411.png]]

теперь мы можем подобрать такое значение $\alpha$ для которого отступ будет ровно 1 

Это по сути:

$M_i$($x_+$) = 1
$M_i$($x_-$) = 1

или так можно записать

![[Pasted image 20250206211928.png]]

Отсюда выводится

![[Pasted image 20250206211950.png]]

Отсюда  мы получаем следующую задачу 

![[Pasted image 20250206212029.png]]
$$\displaylines{\begin{cases} \dfrac{1}{2}||w||^{2}\rightarrow min \\
 y_{i}( w\cdot x_{i} -b) \geq 1\\
 \end{cases} }\\
$$

Нам надо максимизировать  это выражение по по $w$ и $b$, при условии чтобы отступы были больше или равны 1.. (по сути равны 1 те которые находяться на границе полосы, а остальные больше 1)

И эта задача сводиться к задачи минимизации, то есть числитель меняем со знаменателем.

Это задача квадратического программирования. Когда минимизируем квадраты весовы коефф. при линейных ограничениях.

------------



































Задача линейного **SVM** (Support Vector Machine) для линейно разделимой выборки заключается в нахождении гиперплоскости, которая максимально разделяет два класса в пространстве признаков. Рассмотрим, как это выглядит более подробно, с формулами.

#### CHAT GPT  (Компактно)

![[Pasted image 20250202214504.png]]

##### Емко


![[Pasted image 20250202214304.png]]

![[Pasted image 20250202214316.png]]

![[Pasted image 20250202214341.png]]

![[Pasted image 20250202214400.png]]

![[Pasted image 20250202214416.png]]

#### Пример 

![[Pasted image 20250202214644.png]]

![[Pasted image 20250202214730.png]]

![[Pasted image 20250202214755.png]]


-------------

![[Pasted image 20250206010709.png]]


1. Задаем модель классификатора

Все три модели = одно и тоже, просто по разному записаны

![[Pasted image 20250206010827.png]]

Здесь:

- **$x$**— это вектор признаков (например, координаты точки в пространстве).
- **$\omega$** — вектор весов, определяющий направление разделяющей гиперплоскости (или границы).
- **$b$ — смещение (или порог), определяющее положение гиперплоскости относительно начала координат.
- **$sign(z)$— функция знака, которая возвращает +1 или -1 в зависимости от того, положительное или отрицательное значение аргумента.

Вот более подробное объяснение параметров:

1. **$w$** (вектор весов):
    
    - Определяет направление разделяющей гиперплоскости. Вектор весов перпендикулярен гиперплоскости, а его длина влияет на «жесткость» гиперплоскости: чем больше $w$, тем меньше ошибки на тренировочных данных.
    - 
2. **$b$** (сдвиг):
    
    - Смещение гиперплоскости. Влияет на то, где именно проходит разделяющая гиперплоскость, относительно начала координат. Изменяя $b$, мы можем «сдвигать» гиперплоскость вдоль пространства признаков.
    - 
3. **$(w^T)⋅x−b$
    
    - Это линейная комбинация признаков, которая позволяет оценить, с какой стороны гиперплоскости находится точка xxx. Если результат положительный, точка будет классифицирована как принадлежащая одному классу, если отрицательный — к другому
    
4. **$sign$:
    
    - Функция, которая помогает преобразовать результат линейной комбинации в классы. Если результат больше нуля, точка классифицируется как положительная (например, класс +1), если меньше — как отрицательная (класс -1).


![[Pasted image 20250206010905.png]]


![[Pasted image 20250206011603.png]]


Ширина это полосы опирается на граничные образы 1 и 2 класса

Как мы можем рассчитать эту ширину?

Возьмем два образа стоящих на границы

![[Pasted image 20250206012321.png]]


----

![[Pasted image 20250206012336.png]]

L - ширина полосы, нам над её максимизировать - это первое и одно из главных условий

Здесь вводиться ещё одно понятие

margin (отступ)

![[Pasted image 20250206012451.png]]

![[Pasted image 20250206012542.png]]

![[Pasted image 20250206012604.png]]


Ещё раз упростили

![[Pasted image 20250206012649.png]]

Итого

![[Pasted image 20250206012723.png]]

нужно найти минимум для 1 уравнения (ну, а это по сути это оптимизация)
при том что M > 1, кроме образов.


![[Pasted image 20250206012857.png]]


# Почему нельзя использовать градиентный спуск
было на записи
# Метод множителей Лагранжа
Чтобы получить решаемую задачу

Двойственная задача хорошая - Метод множителей Лагранжа