
### Задача оптимизаци

![[Pasted image 20250206142733.png]]

теперь надо её минимизировать, но градиентные методы не подходят. т.к функция потерь хоть  и непрерывная, но не гладкая, производная не сущсевтует в точки перегиба

----
Один из вариантов это субградиентные методы

![[Pasted image 20250206142908.png]]

До точки перегиба берем производную равную -1 
А после равную 0

--------

Но чаще всего задачу решают с помощью квадратичного программироваания

с помощью такого подхода (Возможно далее будет рассмотренно отдельно)
оно реализованно в большей части пакетов.

![[Pasted image 20250206143146.png]]


------
после оптимизации приходят к такой виду уравнения

![[Pasted image 20250206143304.png]]

$\lambda_i$ = числа
$y_i$ - метки класса
$x$ - вектор с н признаками

Если какое i-тое значения $\lambda_i$ = 0, значит оно не используется для вычисления вектора $\omega$

А те которые используются называеются опорными векторами.

Отимальный вектор $\omega$ представляется в виде линейной комбинации наблюдений из обучающщей выборки.

--------

![[Pasted image 20250206143805.png]]

Если для какого либо объекта $\lambda_i$ = 0 ,т о этот объект относиться к неиформативным объектом (они не играют роли их можно отбросить)

Если 0 < $\lambda_i$ < C, $\xi_i$ = 0; $M_i$ (отступ) = 1; значит это опорные граничные объекты
Если  $\lambda_i$ < C, $\xi_i$ > 0; $M_i$ (отступ) < 1;  значит опорные ошибочные объекты

Часто ошибочные объекты используют для спам-фильтров

### Продолжение и пояснение к задачи оптимизации 

Объединяем две формулы

![[Pasted image 20250206230314.png]]

И видим что классфикатор может быть записан в такой форму =>

Здесь мы видим, что классификатор фактически вычисляем взвешенную сумму скалярных произведений только для опорных векторов, а остальные прото отбрасывает

![[Pasted image 20250206230449.png]]

графически это выглядит так 

x1 , x2, x3, x4 - опорные векторы (допустим смещение b = 0)
тогда для произвольного x вектора будет вычсялиться  след линейная комбинация

некий коэфф $\lambda$ уможается на скалярное произведение

![[Pasted image 20250206230830.png]]

это скалярное произведение по сути мы делаем проекции

для произвольного вектора x сначала вектор x1,  потом на x2, x3, x4 (то есть делаем проекции с вектора x на все опорные вектора) 

и затем мы все это суммируем
первые два значения у нас положительные т.к у них класс +1
а остальные два вектора имею класс с меткой -1


то есть начала мы суммируем для одного класса, а потом для другого
![[Pasted image 20250206231136.png]]

после этого вычитаем из одной суммы другую (запись <> - скалярно произведение)

![[Pasted image 20250206231259.png]]

та сумма которая  оказалась наибольшей, тот  знак у нас и получиться

если проекция вектора будет больше на + вектора то у него будет класс +, если больше на - значит класс -

![[Pasted image 20250206231525.png]]

-------
#### Условие Каруша-Кена-Таккера с поиском седловой точки Лангранжа

Условия Каруша-Кена-Таккера (KKT) — это набор условий оптимальности для задач нелинейного программирования, которые могут быть использованы для нахождения **седловых точек** функции Лагранжа при решении задачи оптимизации с ограничениями.

![[Pasted image 20250206223611.png]]
 $$\displaylines{



\begin{cases} \dfrac{1}{2}||w||^{2} + C \sum\limits_{i=1}^{n} \xi_{i} \rightarrow min \\
 y_{i}( w\cdot x_{i} -b) \geq 1 - \xi_i\\
 \end{cases} \\\\\\
} $$
 $$\displaylines{

y_{i}(w\cdot x_{i}- b)\geq 1 - \xi_{i}= g (w,\xi) \tag{1} \\\\
 
}$$
$$\displaylines{

L = f(w, b,  \xi )+ \sum\limits\lambda_{i}~g_{i}(w,\xi)\\\\

\dfrac{\delta L}{\delta w} = \text{если брать производные по всем омегам, то от всех омег производная будет рана нулю,  }\\
\text{кроме производной от $w_i$, там производная от $\dfrac{1}{2}||w||^{2}$ будет равна $w$}\\\\


\text{формулу (1) перепишем в виде }g(w,\xi) = y_{i}(w \cdot x_{i} - b)-1+\xi_i\\\\

\text{теперь деффиринцируем $g$ по $w$, получим $y_{i}\cdot x_{i} \cdot \lambda$ } \\\\\\\

\text{Тогда:}\\

\dfrac{\delta L}{\delta w} =  w +  \sum\limits y_{i}\cdot x_{i}\cdot \lambda_{1, i}
 }$$

###### Подробнее производные 
Используем вторую норму
###### Продолжение
Метод множителей Лагранжа: конспект 2025 года
$$\displaylines{
L(w, \xi, b, \lambda) = f(w,\xi,b) - \lambda_1(y_i(x_{i}\cdot w) - 1 +\xi_{i)} -\lambda_{2}\cdot \xi_i

}$$

$$ \displaylines{

\dfrac{\delta L}{\delta w} = w-\sum \lambda_{1,i} ~ \cdot ~ y_{i}~\cdot~ x_{i}=0\\

\dfrac{\delta L}{\delta b} = \sum\limits \lambda_{1,i}\cdot y_{i}= 0\\

\dfrac{\delta L}{\delta \xi_i} = C-\lambda_{1,i} - \lambda_{2,i} = 0\\

\dfrac{\delta L}{\delta \lambda_{1}} = 1 - y_{i}(w\cdot x_{i} - b) -\xi_{i} = 0\\

\dfrac{\delta L}{\delta \lambda_{2}} = \xi_{i}= 0\\

}$$

**Приравняем все производные к нулю** 
$$\displaylines{
\text{Из первого уравнения получили, что } ~  w  =   \sum\limits y_{i}\cdot x_{i}\cdot \lambda\\
\text{Посчитаем }~ ||w||^{2}= \left(\sum\limits y_{i}\cdot x_{i}\cdot \lambda\right)\cdot \left(\sum\limits y_{i}\cdot x_{i}\cdot \lambda\right) = \sum\limits \sum\limits y_{i} y_{j} (x_{i} \cdot x_{j}) \lambda_{i} \lambda_{j}

}$$
Из третьего получим, что
$$\displaylines{
С = \lambda_{1,i} + \lambda_{2,i}\\

\text{Кси итое по 5-тому уравнению равно нулю, тогда в третьем уравнении:}\\~~~ 1 = y_{i}(x_{i}w-b)\\
\dfrac{1}{y_{i}} = x_{i}w-b\\
b = x_{i}w - \dfrac{1}{y_{i}}
}$$

Тогда хотим минимизировать следующую функцию:

$$\displaylines{
\begin{cases}
\sum\limits\lambda_{i}+ \dfrac{1}{2} \sum\limits \sum\limits y_{i} y_{j} (x_{i} \cdot x_{j}) \lambda_{1,i} \lambda_{1,j} \rightarrow min \\\\

 0\leq\lambda_{i}\leq C \\

\\\sum\limits \lambda_{1,i}\cdot y_{i}= 0 \\
 \\
w  =   \sum\limits y_{i}\cdot x_{i}\cdot \lambda_{1,i}
\end{cases}
}$$

Мы знаем, что некоторые $\lambda_{i}= 0$ , а для некоторых не будут равны 0. $\lambda_{i}= 0$ , если точка находится в глубине области, а  $\lambda_{i} \neq 0$, для точек, которые находятся на границе. 

Поэтому наша омега будет считаться как сумма лямбд и будет задаваться объектами, которые находятся на границе. Плоскости задаются этими объектами и именно эти лямбды, которые не равны нулю называют support vectors

![[Pasted image 20250523003159.png]]

Теперь осталось условие $$\displaylines{

\sum\limits \lambda_{i}\cdot y_{i}= 0 ~~\text{ это производная по $b$ (по байесу), чтобы это условие выполнялось } \\ 
\text{байес был равен нулю}\\

w\cdot x - b = 0 \text{сделаем следующее}\\

x^{*} = [x;~ 1] \text{  приписали единицу к иксам, перешли в пространство на 1 больше }\\
\text{Тогда } \tilde{w} = [w;~ b] \\

\tilde{w} \cdot x^{*} = 0 \\

\text{Тогда производная от } \tilde{w} \cdot x^{*} \text{будет равна нулю и это условие с суммой пропадает}

}$$
$$\displaylines{
\begin{cases}
L = -\sum\limits\lambda_{i}+ \dfrac{1}{2} \sum\limits \sum\limits y_{i} y_{j} (x_{i} \cdot x_{j}) \lambda_{i} \lambda_{j} - \sum\limits\lambda_i \rightarrow min \\\\

 0\leq\lambda_{i}\leq C \\\\
w  =   \sum\limits y_{i}\cdot x_{i}\cdot \lambda_{i}
\end{cases}
}$$
Дальше на каждом шаге оптимизуем L по лямбда в минимум, проверяем выполнение условия  $0\leq\lambda_{i}\leq C$, а в конце просто считаем веса по формуле $w  =   \sum\limits y_{i}\cdot x_{i}\cdot \lambda_{i}$ 

Причём делаем покординатный градиентный спуск мы каждый раз делаем шаг не повсем лямбдам, а по конкретной (каждый раз случайной), чтобы упростить проверку условия.

##### Решение выше позволяет рисовать прямые. Как рисовать кривые?
Заменим скалярное произведение на волшебную функцию, которая будет показывать похожесть векторов 

$$\displaylines{L = - \dfrac{1}{2} \sum\limits \sum\limits y_{i} y_{j} ~ k(x_{i}, x_{j})~ \lambda_{i} \lambda_{j} = \sum\limits\lambda_i \rightarrow min }$$

Варианты для волшебных функций $k$ :
![[Pasted image 20250523010516.png]]
(последняя функция k называется RBF)


kernel trick

Для ядер: можем потребовать, что ядро равно скалярному произведению, от преобразованных векторов $k(x_i,x_j)=\phi(x_{i})\cdot \phi(x_{j})$, тогда вообще ничего не меняется, за исключением того, что к каждому иксу применяем волшебную функцию $\phi$, переходим в другое пространство, где наши точки разделимы и посчитаем в нём svm, где точки будут разделимы, вернёмся в наше пространство, где прямая станет кривой и мы победили. 

![[Pasted image 20250206223651.png]]

![[Pasted image 20250206223705.png]]

![[Pasted image 20250206223723.png]]

Условия Каруша-Кена-Таккера (KKT) позволяют находить оптимальные решения задач с ограничениями, включая задачи с неравенствами и равенствами, через метод Лагранжа. Эти условия включают градиентные условия для переменных, условия для множителей Лагранжа (неотрицательность и комплементарную слабость), а также условия для равенств.

### Двойственная задача

Двойственная задача - задача, которая написана по-другому по отношению к первой, но решение которой будет также решением для первой

То бишь, у нас есть задача минимизации для SVM. Эту задачу можно как-то перевернуть, переписать в другой форме, получить двойственную, решить её и получить искомые значения весов. В этом поможет функция Лагранжа

Для перехода к двойственной задаче используется метод множителей Лагранжа.

Функция Лагранжа позволяет записать систему с ограничениями в виде одной формулы. Каждое ограничение домножается на множитель Лагранжа $α_i$ (или $λ_i$) и прибавляется.

То есть, для такой системы с ограничениями:

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXe5vREdcD_FfgmHfdAXcZIzTwuzBd7DByl5NSzgdTjSAcesisspzgDfS8sWoDxaj1ZwKSdH0a4qyhy2ZeUFH8TP1ngeIPJzrh0xIsKkvrqgKZaLKU5ywmhAAC7yjebsfaeKpC95wg?key=NvVhhl2xI_av0KI_YsKqimqD)

Будет такая функция Лагранжа:

![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXf-dGN72o3uCYEBF0T6E5SZJH08hNQHIH9d_uEnwb28hOzkCTgWNGYwzRNIH0ukomRF5HgxVGbu_w8AiNlPkiFocIKdM_T8CPfbJOC94nwE6hAoHG-5ljh9aUhNF_DqIjYznz5p?key=NvVhhl2xI_av0KI_YsKqimqD)

Двойственная задача здесь - это найти седловую точку этой функции Лагранжа (седловая - значит найти точка будет максимальной по $α_i$, но минимальной по весам θ)

Каждому объекту мы присваиваем "вес" $α_i$ ​ , который показывает, насколько это объект важен для построения зазора:

- Если $a_i$ ​ = 0: объект находится далеко от границы и не влияет на решение.
    
- Если $α_i$​ > 0: объект является опорным вектором и помогает определить положение зазора.
    

Основная идея:

Мы "переворачиваем" задачу: вместо минимизации целевой функции (как в прямой задаче), мы максимизируем другую функцию, которая зависит от $α_i$.

Цель:

Мы хотим найти такие веса $α_i$ ​ , чтобы: зазор был максимально широким. Ошибки (объекты на чужой стороне) были минимальными.

----


![](https://lh7-rt.googleusercontent.com/docsz/AD_4nXcwez3zzK4LYwPkMjVSEi_TbKZJ3msPk-iZ27nZ-suNteaePw0aRM_8S9fRAwhW4IFvHZECo9b9PtCWcMbnJ8vfvvpbMFXbp6NByIZCws0Pb28Y-dFxl2IQ-WVsElp8xnuUvds3WQ?key=NvVhhl2xI_av0KI_YsKqimqD)

