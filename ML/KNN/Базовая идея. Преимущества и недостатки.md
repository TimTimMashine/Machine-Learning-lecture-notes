
Алгоритм **k-Nearest Neighbors (k-NN)** – это метод, который классифицирует новый объект, ориентируясь на **k ближайших соседей** в обучающей выборке.

#### **1. Этап 1: Вычисление расстояния между объектами**

Для определения ближайших соседей необходимо измерить **сходство** между новым объектом (x) и всеми объектами обучающей выборки (X)  

Наиболее распространённые метрики расстояния:

![[Pasted image 20250201172415.png]]

![[Pasted image 20250201172511.png]]

![[Pasted image 20250201172527.png]]

#### **2. Этап 2: Определение k ближайших соседей**

После вычисления расстояний между новым объектом x и всеми объектами обучающей выборки, мы выбираем **k объектов**, расстояние до которых минимально.

Допустим, k=3 Тогда ближайшие соседи:

1. B (Синий)
2. A (Красный)
3. C (Красный)

#### **Этап 3: Присвоение класса (классификация)**

Теперь, когда у нас есть k ближайших соседей, нужно решить, к какому классу отнести новый объект.

В **классификации** применяется **метод голосования** — выбирается класс, встречающийся **чаще всего** среди соседей.

![[Pasted image 20250201173405.png]]

Пояснение к картинке .. формула может выглядеть сложно, но посути мы просто каждому объекту вместо его расстояния ставим его класс и выбираем моду, если это будет список, условно 

(красный, красный, синий, красный)
### Преимущества

- Простота реализации
- Гибкость в выборе метрики расстояния
- Хорошо работает на малых и средних выборках
- Может классифицировать нелинейные данные
- Универсальность (подходит для классификации и регрессии)
### Недостатки
 
- Медленная скорость работы на больших данных (**O(n)** на один запрос)
- Чувствительность к шуму в данных
- Требует нормализации признаков
- Сложность выбора оптимального k
- Высокие затраты памяти (хранит всю обучающую выборку)

----------------------------------------------------------------------------------


