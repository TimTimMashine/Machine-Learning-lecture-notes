В регрессии методом k-ближайших соседей (также называемой сглаживанием k-NN) используется алгоритм k-NN для оценки непрерывных переменных. Один из таких алгоритмов вычисляет взвешенное среднее значение для k ближайших соседей, при этом веса определяются как обратные расстояния до этих соседей. Вот как работает этот алгоритм:

- **Вычисление расстояния**: Сначала необходимо вычислить расстояние между запросом (новой точкой, для которой нужно предсказать значение) и всеми обучающими примерами. В качестве метрики расстояния обычно используется **евклидово расстояние** или **[[Метрика Михаланобиса|метрика Махаланобиса]]**  (если данные имеют корреляцию между признаками).

- **Сортировка по расстоянию**: После вычисления расстояний все обучающие примеры сортируются по возрастанию расстояния от точки запроса (то есть, ближайшие соседи идут первыми).

- **Выбор оптимального числа соседей (k)**: Число соседей, **k**, выбирается с использованием эвристического подхода. Обычно это делается с помощью **кросс-валидации**, когда тестируются различные значения k, и выбирается то, которое минимизирует **среднеквадратичную ошибку (RMSE)** на валидационном наборе. Малые значения k могут привести к переобучению (высокая изменчивость), а большие — к недообучению (высокий смещение)

- **Взвешенное среднее**: Вместо простого среднего вычисляется **взвешенное среднее** для k ближайших соседей, где вес каждого соседа обратно пропорционален его расстоянию от точки запроса. То есть, чем ближе сосед, тем больше его вклад в предсказание.

![[Pasted image 20250202164105.png]]

##### Основные преимущества:

- **Точность предсказаний**: Такой метод часто дает более точные предсказания по сравнению с простым средним, особенно когда есть значительные различия в расстояниях между соседями.
- **Локальное сглаживание**: Путем взвешивания ближайших точек алгоритм сглаживает шум в тех областях, где данные более однородны.

##### Важные моменты:

- **Выбор метрики расстояния**: Обычно используется евклидово расстояние, но в некоторых случаях метрики, такие как метрика [[Метрика Михаланобиса|Махаланобиса]] или Минковского, могут дать лучшие результаты в зависимости от структуры данных.
- **Влияние взвешивания по расстоянию**: Если данные содержат много шума, то чрезмерное внимание к ближайшим соседям может привести к переобучению. Метод взвешенного расстояния лучше работает, когда в данных присутствуют локальные закономерности.

Этот метод особенно полезен в случаях, когда локальные тренды или паттерны в данных важны, и предполагается, что точки, расположенные близко друг к другу в пространстве признаков, имеют схожие значения целевой переменной.

### Пример

![[Pasted image 20250202170351.png]]


![[Pasted image 20250202170408.png]]

##### **Пометка**

![[Pasted image 20250202170535.png]]

![[Pasted image 20250202170611.png]]