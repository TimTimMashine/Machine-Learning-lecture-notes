
### Идея 
Проблема [[Adagrad, нормирование градиента|Adagrad]] в том, что v  всегда только увеличивается -> стремится к бесконечности и в какой-то мы будем отнимать от икса ноль.

Идея в том, что теперь мы можем частично "забывать" предыдущие значения v. Они всё ещё влияют на нормирование вектора градиентов, но с каждым шагом всё слабее. 

И если градиент уменьшается, то и v будет всё меньше и меньше, а значит и смещаться мы будем всё медленнее и медленнее.
$$ 
\displaylines{

\text{grad} \, f(\vec{x}) = \left\{ \frac{\partial f}{\partial x_i} \right\}_{i=1}^n \\


\vec{x} = \{x_1, x_2, x_3, x_4, \dots\} \\

\text{RMSprop}\\

\vec{v} = 0, \quad \vec{x_0} = 0, \\
 \quad \alpha \in [0,1], \quad \alpha = 0,9 \\

\text{for } i = 1 \text{ to } n:
\quad \\

 ~~~~~~~~~~~~~~~~ g = \text{grad} \, (f(x_{i-1})) \\
~~~~~~~~~~~~~~~~~~~~~~~ v = \alpha\cdot v +  (1-\alpha) \cdot g ^ 2 \\

~~~~~~~~~~~~~~~~~~~~~~~~x_{i}= x_{i-1} - \gamma\cdot \dfrac{g}{\sqrt{v + \varepsilon}} \\

}


$$
![[Pasted image 20250414123742.png]]

Постановка проблемы

![[Pasted image 20250204222928.png]]

Когда мы ищем частные производные, они всегда могут быть очень разные в абсолютных значения и сильно отличаться друг от друга.

**Неравномерное обновление параметров:**  
Если один параметр имеет очень большую производную, а другой – маленькую, то при одном и том же шаге (learning rate) обновления будут слишком большими для одних и слишком маленькими для других. Это может привести к тому, что алгоритм либо «перескакивает» через оптимальное значение для чувствительных параметров, либо очень медленно корректирует менее чувствительные.

----------
RMSprop (Root Mean Square Propagation) — это адаптивный метод оптимизации, который автоматически подбирает индивидуальные темпы обучения для каждого параметра, учитывая историю градиентов. Он особенно полезен для задач, где градиенты могут сильно меняться по масштабу или иметь шум. Рассмотрим этот метод подробнее.

![[Pasted image 20250204224453.png]]

------------
**Отличие от Adagrad**

Вместо накопления суммы квадратов градиентов за всё время обучения, RMSprop использует экспоненциальное скользящее среднее квадратов градиентов

Это позволяет «забывать» старую информацию и учитывать только недавнюю историю градиентов, что предотвращает чрезмерное уменьшение effective learning rate.

----------------
#### Экспоненциальное скользящее среднее (Важный момент)

В данном контексте слово «экспоненциальное» относится к способу вычисления скользящего среднего, где веса прошлых значений уменьшаются по экспоненциальному закону. То есть, при расчёте текущего среднего значение последнего наблюдения имеет больший вклад, а вклад старых наблюдений экспоненциально затухает.

![[Pasted image 20250204235843.png]]


![[Pasted image 20250204235919.png]]
##### Объяснение с видео

Инициализируем начальное состояние часто для G

Вычисляем G - скользящее экспонициальное среднее для квадратов градиентов

![[Pasted image 20250204224606.png]]

 добавляем эпсилон для того чтобы не делить на 0 в случае чего

##### Другое объяснение

![[Pasted image 20250204224836.png]]
![[Pasted image 20250204224932.png]]
