
### Стохастический градиентный спуск (SGD)

(Stochastic Gradient Descent, SGD) — это разновидность градиентного спуска, в которой мы **обновляем параметры модели после каждого отдельного примера**, а не после просмотра всей выборки или мини-батча.

![[Pasted image 20250204202423.png]]

Когда мы вычисляем градиент, не по 1 образу, а по пакетам образов, то это назвается SGD с мини-батчем (пример в конце)
#### Инфа с видео
----------------

C SGD, мы заменяем градиент на псевдоградиент, псевдоградиент должен образовывать острый угол в n - мерном признаковом пространстве с истиным градиентом, в среднем, но не всегда

![[Pasted image 20250204202237.png]]

Формула SGD для k - образа 

![[Pasted image 20250204202723.png]]

его можно представить с помощью такого псевдокода

![[Pasted image 20250204202845.png]]

Пересчет функционала качества происходит по экспонециальному скользящему среднему в пункте 7. так как если по обычной формуле пересчитывать то это будет очень долго, столько же ресурсов и времени затратит.

![[Pasted image 20250204203736.png]]
Это просто формула суммы средних ошибок для кадого параметра, то есть формула функционала качества

Отсюды выводиться эта формула
![[Pasted image 20250204203928.png]]

Отсюда, конечная, формула

![[Pasted image 20250204204043.png]]

### SGD с мини-батчем

Недостатки простого SGD, это неравномерная сходимость к локальному минимуму, это компенсируют когда делают антиградиент не по одному образу, а по пакетам образов (мини-батчи) 

![[Pasted image 20250204204625.png]]

### SAG

![[Pasted image 20250204205200.png]]

Не достаток это то что надо хранить все градиенты, а это занимает памяти много