### Adam (Адаптивный момент)
Идея: взяли идею с моментами: момент накапливается от градиента.

И объединили с идеей, что v для нормирования векторов тоже должно накапливаться.


$$ 
\displaylines{

\text{grad} \, f(\vec{x}) = \left\{ \frac{\partial f}{\partial x_i} \right\}_{i=1}^n \\


\vec{x} = \{x_1, x_2, x_3, x_4, \dots\} \\

\text{RMSprop}\\

\vec{v} = 0, \quad \vec{x_{0} = 0}, \quad \vec{m} = 0 \\
 \quad \beta_{1}, \beta_{2}  \in [0,1] \\
  \beta_{1} = 0,999, \beta_{2} = 0,9 \\

\text{for } i = 1 \text{ to } n:
\quad \\

 ~~~~~~~~~~~~~~~~ g = \text{grad} \, (f(x_{i-1})) \\
~~~~~~~~~~~~~~~~~~~~~~~~~~~m = \beta_{1}\cdot m + (1-\beta_{1})\cdot g \\
~~~~~~~~~~~~~~~~~~~~~~~~~ v = \beta_{2}\cdot v +  (1-\beta_2) \cdot g ^ 2 \\


~~~~~~~~~~~~~~~~~~~~~~~~x_{i}= x_{i-1} - \gamma\cdot \dfrac{m}{\sqrt{v + \varepsilon}} \\

}


$$
![[Pasted image 20250414130253.png]]
Но m и v - берутся волшебными, потому что Adam медленно разгоняется. Поэтому m и v cначала будут большими, потому что в знаменателе 1 - бета. Знаментель маленький -> число большое. Но с течением итераций (t) знаменатель будет стремиться к единице и дробь будет уменьшаться. 
$$ 
\displaylines{

\text{grad} \, f(\vec{x}) = \left\{ \frac{\partial f}{\partial x_i} \right\}_{i=1}^n \\


\vec{x} = \{x_1, x_2, x_3, x_4, \dots\} \\

\text{RMSprop}\\

\vec{v} = 0, \quad \vec{x_{0} = 0}, \quad \vec{m} = 0 \\
 \quad \beta_{1}, \beta_{2}  \in [0,1] \\
  \beta_{1} = 0,999, \beta_{2} = 0,9 \\

\text{for } i = 1 \text{ to } n:
\quad \\

 ~~~~~~~~~~~~~~~~ g = \text{grad} \, (f(x_{i-1})) \\
~~~~~~~~~~~~~~~~~~~~~~~~~~~m = \beta_{1}\cdot m + (1-\beta_{1})\cdot g \\
~~~~~~~~~~~~~~~~~~~~~~~~~ v = \beta_{2}\cdot v +  (1-\beta_2) \cdot g ^ 2 \\

~~~~~\tilde{m} = \dfrac{m}{1 - \beta_{1}^{t}}\\

~~~~\tilde{v} = \dfrac{v}{1 - \beta_{2}^{t}}\\

~~~~~~~~~~~~~~~~~~~~~~~~x_{i}= x_{i-1} - \gamma\cdot \dfrac{\tilde{m}}{\sqrt{\tilde{v} + \varepsilon}} \\

}


$$



**Adam** (Adaptive Moment Estimation) — это один из самых популярных методов оптимизации, используемых при обучении нейронных сетей и других моделей машинного обучения. Adam сочетает в себе идеи **momentum** (накапливание экспоненциального скользящего среднего градиентов) и **RMSprop** (экспоненциальное скользящее среднее квадратов градиентов), чтобы обеспечить адаптивное изменение шага обучения для каждого параметра.

Сочетание RMSprop и алгоритма с моментом

![[Pasted image 20250204230110.png]]

сначала вычисляем экспоницальные скользящие средние

![[Pasted image 20250204230215.png]]

нормируем их, что на нчальных интерациях измения были больше, а на поздних меньше

![[Pasted image 20250204230258.png]]

------------
#### Другое объяснение

![[Pasted image 20250205005247.png]]

![[Pasted image 20250205005438.png]]

![[Pasted image 20250205005511.png]]

![[Pasted image 20250205010257.png]]

## Итог

**Adam** — это алгоритм оптимизации, который:

- Вычисляет экспоненциальные скользящие средние первого и второго моментов градиентов.
- Применяет корректировку смещения для получения точных оценок.
- Обновляет параметры, используя адаптивный шаг, зависящий от этих моментов.

Благодаря этим механизмам Adam часто показывает быструю и стабильную сходимость, что делает его выбором по умолчанию во многих современных задачах глубокого обучения.