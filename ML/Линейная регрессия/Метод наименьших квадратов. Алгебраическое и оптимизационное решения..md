![[Pasted image 20241104182832.png]]


Метод наименьших квадратов (МНК) — это способ нахождения коэффициентов линейной регрессии, минимизирующий расхождение между предсказанными и фактическими значениями.

По оси Y - чаще всего откладывают зависимую переменную
По оси X - чаще всего откладывают переменную предиктор

y = b0 + b1x 

b0 = intercept или свободный член (отвечает за то где наша линия пересекает ось X)
b1 = slope  (отвечает за направление линии и за угол наклона который образруют прямая с осью X)

Мы хотим чтобы наша линия проходила через самый центр рассеивания
Мы хотим подобрать intercept и slope таким образом чтобы наша линия максимально адекватно отражала связь двух кол-в переменных между собой

----------------------------------------------------------
### Метод наименьших квадратов (OLS) - cо степика

![[Pasted image 20241104183314.png]]

Что такое остатки? (По сути - это разница между отклоняемыми значенияи и предсказанными)
И можно посчитать такую переменную как сумма квадратов остатков.

Так вот метод МНК позволят подобрать b0 и b1 таким образом чтобы суммма кв. остатков была минимимальна

![[Pasted image 20241104184801.png]]

Формулы для расчета коэффицентов регресии

Здесь r xy - коэфф. корреляции определяет направление нашей линии

![[Pasted image 20241104184835.png]]

Значение b1 - будет также равно значению угла между прямой и осью x
y ^ = b0 + b1 * x  : По сути по такой формуле мы предсказываем точки, которые бы легли на нашу прямую 

![[Pasted image 20241104185334.png]]

----------------------------------------------------
t - критерий здесь помогает проверить гипотезу о взаимосвязи двух переменных

![[Pasted image 20241104191352.png]]

SE - стандартная ошибка, измеряет, насколько **точно** среднее значение, вычисленное по выборке, оценивает среднее генеральной совокупности

sd - станадртное отклонение, измеряет насколько сильно отдельные наблюдения **разбросаны** вокруг среднего значения в генеральной или выборочной совокупности.

---------------
Регрессия с лекций

### Регрессия для одной независимой переменной

Одномерный регрессионный анализ-используется для изучения связи между двумя количественными переменными: одна переменная является зависимой(y), а другая независимой (predict или x).

Задача одномерной регрессии - найти уравнение линейной зависимости между этими двумя переменными

Линия регрессии-прямая, которая наилучшим образом описывает зависимость между двумя переменными в одномерной линейной регрессии.

y = a+ bx где a-свободный член (intersept), точка пересечения прямой и оси y (когда все переменные равны нулю); b –характеризует угол наклона (tg); x -это известное значение независимой переменной

intersept - отвечает за это где пересечет прямая ось oy, slope - за то обратная или прямая будет корреляция

![[Pasted image 20250119161331.png]]
#### Мера качества подгонки прямой

![[Pasted image 20250119161353.png]]

Далее разные типы вариантов (стандартныйы второй вариант)

![[Pasted image 20250119161530.png]]

### Метод наименьших квадратов (OLS) - с универа

![[Pasted image 20250119161714.png]]
(Здесь мы вычитаем уранение прямой которое дает нам (y) прямой из  - отклоняемного от прямой yi и получаем остаток и так с каждой точкой, причем мы каждый остаток возводим в квадрат, а потом все остатки суммируем)

**Основная задача**
**Нужно найти прямую (тренд), меньше всего отклоняющуюся от заданных точек **

У прямой есть 2 коэффицента которые нам надо найти (a - интерсепт и b - слоуп)

![[Pasted image 20250119162128.png]]

По сути дальше мы берем частные производные по a, b 

![[Pasted image 20250119163651.png]]

**зачем нам это надо спросишь?

*Частные производные показывают, как изменяется функция относительно одного из параметров при фиксированном значении других параметров. 

1. И мы берем частные производные по a, b и получаем две функции частных производных
   ---
2. Потом приравниваем их к нулю (это если мы хотим найти при каких значениях функции частных производных по a, b будут иметь нулевую скорость изменения, то есть их экстремумы, но неизвестно максимум это будет или минимум)
   ---
3. Далее это можно из этого можно сделать систему уравнений


**Далее мы по сути выносим a, b

![[Pasted image 20250119173304.png]]

**Небольшой комментарий:

![[Pasted image 20250119173329.png]]

-------------------------
### Условия применения линейной модели регрессии

1. Линейная взаимосвязь x и y
2. Нормальное распределение остатков
3. Гомоскедастичность - постоянная изменчивость остатков на всех уровнях независимой переменной. Если остатки имеют постоянную дисперсию по всем значениям независимой переменной, то говорят, что модель гомоскедастична

![[Pasted image 20250119173903.png]]

### Основные гипотезы линейной модели регрессии

![[Pasted image 20250119174207.png]]

### Статистические свойства оценок. Проверка гипотез и доверительные интервалы для коэффициентов регрессии

--------------------
Пояснение ко 2 задачи

![[Pasted image 20250120012339.png]]

![[Pasted image 20250120022557.png]]

### Алгебраическое решение

![[Pasted image 20250205151418.png]]

![[Pasted image 20250205151431.png]]
### 

Оптмизационное решение

![[Pasted image 20250205151337.png]]

